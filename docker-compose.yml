version: '3'

services:
  nginx:
    container_name: nginx
    build: ./nginx
    volumes:
      - static_volume:/var/www/static
      - media_volume:/var/www/media
      - admin_volume:/var/www/static/admin
    depends_on:
      - movies
      - auth
      - admin
    ports:
      - "8000:80"
      - "443:443"
    restart: on-failure

  admin:
    container_name: admin
    build: ./admin
    volumes:
      - admin_static:/var/www/static/admin
    expose:
      - "8000"
    env_file:
      - .env
    depends_on:
      - postgres
    restart: on-failure

  movies:
    container_name: movies
    build: ./movies
    expose:
      - "8000"
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    depends_on:
      - postgres
      - elasticsearch-node1
      - elasticsearch-node2
      - redis
      - jaeger
    restart: on-failure

  auth:
    container_name: auth
    build: ./auth
    expose:
      - "8000"
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    depends_on:
      - postgres
      - redis
      - jaeger
    restart: on-failure

  etl-elasticsearch:
    container_name: etl-elasticsearch
    build: ./etl_elasticsearch
    env_file:
      - .env
    depends_on:
      - postgres
      - elasticsearch-node1
      - elasticsearch-node2
      - redis
    restart: on-failure

  postgres:
    container_name: postgres
    image: postgres:${POSTGRES_VERSION}
    env_file:
      - .env
    volumes:
      - ./postgres/:/docker-entrypoint-initdb.d/
    ports:
      - "5432:5432"
    restart: on-failure

  elasticsearch-node1:
    image: elasticsearch:${STACK_VERSION}
    container_name: elasticsearch-node1
    env_file:
      - .env
    environment:
      - node.name=elasticsearch-node1
      - discovery.seed_hosts=elasticsearch-node1,elasticsearch-node2
      - cluster.initial_master_nodes=elasticsearch-node1
      - cluster.name=${ELASTIC_CLUSTER_NAME}
      - bootstrap.memory_lock=${MEMORY_LOCK}
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=false
#      - xpack.license.self_generated.type=basic
    ulimits:
      memlock:
        soft: -1
        hard: -1
    restart: on-failure

  elasticsearch-node2:
    image: elasticsearch:${STACK_VERSION}
    container_name: elasticsearch-node2
    env_file:
      - .env
    environment:
      - node.name=elasticsearch-node2
      - discovery.seed_hosts=elasticsearch-node1,elasticsearch-node2
      - cluster.initial_master_nodes=elasticsearch-node1
      - cluster.name=${ELASTIC_CLUSTER_NAME}
      - bootstrap.memory_lock=${MEMORY_LOCK}
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=false
#      - xpack.license.self_generated.type=basic
    depends_on:
      - elasticsearch-node1
    ulimits:
      memlock:
        soft: -1
        hard: -1
    restart: on-failure

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:v0.7.0
    ports:
      - ${KAFKA_PORT_UI}:${KAFKA_PORT_UI}
    environment:
      - KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_NAME=kraft
    restart: on-failure

  kafka:
    container_name: kafka
    image: bitnami/kafka:${KAFKA_VERSION}
    ports:
      - ${KAFKA_PORT}:${KAFKA_PORT}
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:${KAFKA_PORT_CONTROLLER}
      - KAFKA_KRAFT_CLUSTER_ID=${KAFKA_KRAFT_CLUSTER_ID}
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:${KAFKA_PORT_PLAINTEXT},CONTROLLER://:${KAFKA_PORT_CONTROLLER},EXTERNAL://:${KAFKA_PORT}
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:${KAFKA_PORT_PLAINTEXT},EXTERNAL://kafka:${KAFKA_PORT}
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
    volumes:
      - kafka_data:/bitnami/kafka
    restart: on-failure

  zookeeper:
    image: zookeeper:${ZOOKEEPER_VERSION}
    container_name: zookeeper
    hostname: zookeeper
    restart: on-failure

  clickhouse-node:
    image: clickhouse/clickhouse-server:${CLICKHOUSE_VERSION}
    container_name: clickhouse-node
    hostname: clickhouse-node
    ports:
      - ${CLICKHOUSE_PORT}:${CLICKHOUSE_PORT}
      - ${CLICKHOUSE_CLIENT_PORT}:${CLICKHOUSE_CLIENT_PORT}
    volumes:
      - ./ugc/data/node1:/etc/clickhouse-server
    env_file:
      - .env
    depends_on:
      - zookeeper
    restart: on-failure

  etl-clickhouse:
    build: ./etl_clickhouse
    container_name: etl-clickhouse
    command: bash -c "python main.py"
    env_file:
      - ./.env
    depends_on:
      - clickhouse-node
      - postgres
    restart: on-failure

  redis:
    container_name: redis
    image: redis:latest
    volumes:
      - ./core/redis.conf:/redis.conf:rw
    command: [ "redis-server", "/redis.conf" ]
    restart: on-failure

  jaeger:
    container_name: jaeger
    image: jaegertracing/all-in-one:${JAEGER_VERSION}
    ports:
      - ${JAEGER_PORTS_1}
      - ${JAEGER_PORTS_2}
    environment:
      - LOG_LEVEL=debug
    env_file:
      - .env
    restart: on-failure

  ugc:
    container_name: ugc
    build: ./ugc
    ports:
      - ${FLASK_PORTS}
    env_file:
      - .env
    depends_on:
      - clickhouse-node
      - kafka
    restart: on-failure
    volumes:
      - ./logs:/logs:rw
    logging:
      driver: "json-file"
      options:
        max-file: "10"
        max-size: "10m"

  event-generator:
    container_name: event-generator
    build: ./event_generator
    ports:
      - ${GENERATOR_PORTS}
    env_file:
      - .env
    depends_on:
      - postgres
      - kafka

  etl-kafka:
    container_name: etl-kafka
    build: ./etl_kafka
    ports:
      - ${ETL_KAFKA_PORTS}
    env_file:
      - .env
    depends_on:
      - clickhouse-node
      - kafka

  etl-mongo:
    container_name: etl-mongo
    build: ./etl_mongo
    env_file:
      - .env
    depends_on:
      - mongo-init
      - kafka

  locust:
    container_name: locust
    image: locustio/locust:master
    volumes:
      - ./locust:/locust
    env_file:
      - .env
    ports:
      - ${LOCUST_PORT}:${LOCUST_PORT}
    command: -f /locust/locustfile.py --master -H http://${FLASK_HOST}:${FLASK_PORT}

  locust-worker:
    container_name: locust-worker
    image: locustio/locust:master
    volumes:
      - ./locust:/locust
    env_file:
      - .env
    command: -f /locust/locustfile.py --worker --master-host ${LOCUST_HOST}

  logstash:
      container_name: logstash
      image: logstash:${STACK_VERSION}
      volumes:
        - ./configs/logstash/config.yml:/usr/share/logstash/config/logstash.yml:ro
        - ./configs/logstash/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro
        - ./configs/logstash/pipelines:/usr/share/logstash/config/pipelines:ro
      env_file:
        - .env
      environment:
        LS_JAVA_OPTS: "-Xmx512m -Xms512m"
      ports:
        - ${LOGSTASH_PORTS1}
        - ${LOGSTASH_PORTS2}
        - ${LOGSTASH_PORTS3}
      depends_on:
        - elasticsearch-node1

  kibana:
    container_name: kibana
    image: kibana:${STACK_VERSION}
    depends_on:
      - elasticsearch-node1
    volumes:
      - ./configs/kibana/config.yml:/usr/share/kibana/config/kibana.yml:ro
    env_file:
      - .env
    ports:
      - ${KIBANA_PORTS}

  filebeat:
    container_name: filebeat
    image: elastic/filebeat:${STACK_VERSION}
    command: filebeat -e -strict.perms=false
    platform: linux/amd64
    volumes:
      - ./configs/filebeat/config.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs:/logs:ro
    env_file:
      - .env
    depends_on:
      - elasticsearch-node1

  mongo1:
    container_name: mongo1
    image: mongo:6
    restart: on-failure
    expose:
      - 27017
    ports:
      - "27017:27017"
    entrypoint: mongod --replSet myReplicaSet --bind_ip localhost,mongo1

  mongo2:
    container_name: mongo2
    image: mongo:6
    restart: on-failure
    expose:
      - 27017
    ports:
      - "27018:27017"
    entrypoint: mongod --replSet myReplicaSet --bind_ip localhost,mongo2
    depends_on:
      - mongo1

  mongo3:
    container_name: mongo3
    image: mongo:6
    restart: on-failure
    expose:
      - 27017
    ports:
      - "27019:27017"
    entrypoint: mongod --replSet myReplicaSet --bind_ip localhost,mongo3
    depends_on:
      - mongo1

  mongo-init:
    container_name: mongo-init
    image: mongo:6
    restart: on-failure
    volumes:
      - ./mongo/rs-init.sh:/scripts/rs-init.sh
    entrypoint: [ "bash", "/scripts/rs-init.sh" ]
    depends_on:
      - mongo1
      - mongo2
      - mongo3

  notifications:
    container_name: notifications
    build: ./notifications
    expose:
      - "8000"
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    depends_on:
      - jaeger
      - notifications-db
    restart: on-failure

  notifications-db:
    container_name: notifications-db
    image: postgres:${NOTIFICATIONS_DB_VERSION}
    env_file:
      - .env
    volumes:
      - notifications_db_data:/var/lib/postgresql/data
    ports:
      - ${NOTIFICATIONS_DB_PORTS}
    expose:
      - ${NOTIFICATIONS_DB_PORT}
    command: -p ${NOTIFICATIONS_DB_PORT}
    environment:
        POSTGRES_DB: ${NOTIFICATIONS_DB_NAME}
        POSTGRES_USER: ${NOTIFICATIONS_DB_USER}
        POSTGRES_PASSWORD: ${NOTIFICATIONS_DB_PASSWORD}
    restart: on-failure

  rabbitmq:
    container_name: rabbitmq
    image: rabbitmq:${RABBIT_MQ_VERSION}-management-alpine
    ports:
      - ${RABBIT_MQ_PORTS}
      - ${RABBIT_MQ_AMQP_PORTS}
    expose:
      - ${RABBIT_MQ_PORT}
    hostname: ${RABBIT_MQ_HOST}
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBIT_MQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBIT_MQ_PASSWORD}
    restart: on-failure

  rabbitmq-init:
    container_name: rabbitmq-init
    image: rabbitmq:${RABBIT_MQ_VERSION}-management
    volumes:
      - ./rabbitmq/init.sh:/scripts/init.sh
    entrypoint: [ "bash", "/scripts/init.sh" ]
    environment:
      RABBIT_MQ_HOST: ${RABBIT_MQ_HOST}
      RABBIT_MQ_PORT: ${RABBIT_MQ_PORT}
      RABBIT_MQ_TASKS_KEY: ${RABBIT_MQ_TASKS_KEY}
      RABBIT_MQ_NOTIFICATIONS_KEY: ${RABBIT_MQ_NOTIFICATIONS_KEY}
      RABBIT_MQ_ENRICHED_KEY: ${RABBIT_MQ_ENRICHED_KEY}
      RABBIT_MQ_TO_SENDING_KEY: ${RABBIT_MQ_TO_SENDING_KEY}
      RABBIT_MQ_EXCHANGE: ${RABBIT_MQ_EXCHANGE}
      RABBIT_MQ_TASKS_QUEUE: ${RABBIT_MQ_TASKS_QUEUE}
      RABBIT_MQ_NOTIFICATIONS_QUEUE: ${RABBIT_MQ_NOTIFICATIONS_QUEUE}
      RABBIT_MQ_ENRICHED_QUEUE: ${RABBIT_MQ_ENRICHED_QUEUE}
      RABBIT_MQ_TO_SENDING_QUEUE: ${RABBIT_MQ_TO_SENDING_QUEUE}
    depends_on:
      - rabbitmq
    restart: on-failure

  etl-notifications-tasks:
    container_name: etl-notifications-tasks
    build: ./etl-notifications-tasks
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    restart: always

  ws:
    container_name: ${WEBSOCKET_HOST}
    build: ./ws
    env_file:
      - .env
    ports:
      - ${WEBSOCKET_PORTS}
    expose:
      - ${WEBSOCKET_PORT}

  worker-create:
    container_name: worker-create
    build: ./worker-create
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    depends_on:
      - rabbitmq
    restart: always

  worker-send:
    container_name: worker-send
    build: ./worker-send
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
      - ./mailings:/app/mailings:rw
    depends_on:
      - rabbitmq
    restart: always

  worker-enrich:
    container_name: worker-enrich
    build: ./worker-enrich
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
    depends_on:
      - rabbitmq
    restart: always

  worker-pre-send:
    container_name: worker-pre-send
    build: ./worker-pre-send
    env_file:
      - .env
    volumes:
      - ./logs:/logs:rw
      - ./mailings:/app/mailings:rw
    depends_on:
      - rabbitmq
    restart: always

volumes:
    static_volume:
    media_volume:
    admin_volume:
    admin_static:
    elasticsearch_data:
    redis_data:
    postgres_data:
    kafka_data:
    logs:
    notifications_db_data:
